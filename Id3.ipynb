{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsTnBzoLPGwSUDOk9wWJsi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreevatsava007/ML-CONCLAVE/blob/main/Id3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "WbskDryItQhh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a simple dataset with two features: Outlook and Temperature, and a target variable: PlayTennis\n",
        "# Outlook: Sunny=0, Overcast=1, Rainy=2\n",
        "# Temperature: Hot=0, Mild=1, Cool=2\n",
        "# PlayTennis: No=0, Yes=1\n",
        "data = np.array([\n",
        "    [0, 0, 0],  # Sunny, Hot, No\n",
        "    [0, 1, 0],  # Sunny, Mild, Yes\n",
        "    [1, 0, 1],  # Overcast, Hot, Yes\n",
        "    [2, 2, 1],  # Rainy, Cool, No\n",
        "    [2, 1, 1],  # Rainy, Mild, Yes\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define a function to calculate entropy\n",
        "def calculate_entropy(y):\n",
        "    unique_labels, counts = np.unique(y, return_counts=True)\n",
        "    probabilities = counts / len(y)\n",
        "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
        "    return entropy"
      ],
      "metadata": {
        "id": "r_G5LJxhtav4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define a function to calculate information gain\n",
        "def calculate_information_gain(X, y, feature_index):\n",
        "    # Calculate entropy before the split\n",
        "    entropy_before_split = calculate_entropy(y)\n",
        "\n",
        "    # Calculate entropy after the split\n",
        "    unique_values, counts = np.unique(X[:, feature_index], return_counts=True)\n",
        "    weighted_entropy_after_split = 0\n",
        "    for value, count in zip(unique_values, counts):\n",
        "        subset_y = y[X[:, feature_index] == value]\n",
        "        weighted_entropy_after_split += (count / len(X)) * calculate_entropy(subset_y)\n",
        "\n",
        "    # Calculate information gain\n",
        "    information_gain = entropy_before_split - weighted_entropy_after_split\n",
        "    return information_gain"
      ],
      "metadata": {
        "id": "2P2c_z04tey_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the ID3 algorithm\n",
        "def id3(X, y, feature_names):\n",
        "    if len(np.unique(y)) == 1:\n",
        "        # If all instances have the same class label, return a leaf node with that label\n",
        "        return y[0]\n",
        "\n",
        "    if X.shape[1] == 0:\n",
        "        # If there are no more features to split on, return the most common class label\n",
        "        return np.argmax(np.bincount(y))\n",
        "\n",
        "    # Calculate information gain for each feature\n",
        "    information_gains = [calculate_information_gain(X, y, i) for i in range(X.shape[1])]\n",
        "\n",
        "    # Choose the feature with the highest information gain\n",
        "    best_feature_index = np.argmax(information_gains)\n",
        "    best_feature_name = feature_names[best_feature_index]\n",
        "\n",
        "    # Create a decision tree node with the best feature\n",
        "    tree = {best_feature_name: {}}\n",
        "\n",
        "    # Recursively build the tree\n",
        "    unique_values = np.unique(X[:, best_feature_index])\n",
        "    for value in unique_values:\n",
        "        subset_indices = np.where(X[:, best_feature_index] == value)[0]\n",
        "        subset_X = X[subset_indices]\n",
        "        subset_y = y[subset_indices]\n",
        "        subtree = id3(subset_X, subset_y, feature_names)\n",
        "        tree[best_feature_name][value] = subtree\n",
        "\n",
        "    return tree"
      ],
      "metadata": {
        "id": "1zMJGjOGtifh"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define feature names\n",
        "feature_names = ['Outlook', 'Temperature']\n"
      ],
      "metadata": {
        "id": "mUpdt0NdtmCX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and target variable\n",
        "X = data[:, :-1]\n",
        "y = data[:, -1]"
      ],
      "metadata": {
        "id": "Iz8E-vggtodi"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the decision tree\n",
        "decision_tree = id3(X, y, feature_names)"
      ],
      "metadata": {
        "id": "8Xo48Ubhts3c"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the decision tree\n",
        "print(\"Decision Tree:\")\n",
        "print(decision_tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9K9CZs5tvEN",
        "outputId": "3bb5f0b4-b35c-4c26-e690-be12442d2e03"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree:\n",
            "{'Outlook': {0: 0, 1: 1, 2: 1}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "We start by defining a simple dataset with two features (Outlook and Temperature) and a target variable (PlayTennis).\n",
        "The calculate_entropy function calculates the entropy of a given set of class labels.\n",
        "The calculate_information_gain function calculates the information gain for a given feature.\n",
        "The id3 function implements the ID3 algorithm recursively. It selects the best feature to split on at each node based on the highest information gain and builds the decision tree accordingly.\n",
        "Finally, we print the resulting decision tree."
      ],
      "metadata": {
        "id": "k1LYcQltuJKA"
      }
    }
  ]
}